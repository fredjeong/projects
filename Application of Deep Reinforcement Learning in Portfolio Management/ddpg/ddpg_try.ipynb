{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque, namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "from mplfinance.original_flavor import candlestick_ohlc\n",
    "import matplotlib.dates as mpl_dates\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TradingGraph:\n",
    "    # A crypto trading visualization using matplotlib made to render custom prices which come in following way:\n",
    "    # Date, Open, High, Low, Close, Volume, net_worth, trades\n",
    "    # call render every step\n",
    "    def __init__(self, render_range):\n",
    "        self.volume = deque(maxlen=render_range)\n",
    "        self.net_worth = deque(maxlen=render_range)\n",
    "        self.render_data = deque(maxlen=render_range)\n",
    "        self.render_range = render_range\n",
    "\n",
    "        # We are using the style ‘ggplot’\n",
    "        plt.style.use('ggplot')\n",
    "        # close all plots if there are open\n",
    "        plt.close('all')\n",
    "        # figsize attribute allows us to specify the width and height of a figure in unit inches\n",
    "        self.fig = plt.figure(figsize=(16,8)) \n",
    "\n",
    "        # Create top subplot for price axis\n",
    "        self.ax1 = plt.subplot2grid((6,1), (0,0), rowspan=5, colspan=1)\n",
    "        \n",
    "        # Create bottom subplot for volume which shares its x-axis\n",
    "        self.ax2 = plt.subplot2grid((6,1), (5,0), rowspan=1, colspan=1, sharex=self.ax1)\n",
    "        \n",
    "        # Create a new axis for net worth which shares its x-axis with price\n",
    "        self.ax3 = self.ax1.twinx()\n",
    "\n",
    "        # Formatting Date\n",
    "        self.date_format = mpl_dates.DateFormatter('%d-%m-%Y')\n",
    "        \n",
    "        # Add paddings to make graph easier to view\n",
    "        #plt.subplots_adjust(left=0.07, bottom=-0.1, right=0.93, top=0.97, wspace=0, hspace=0)\n",
    "\n",
    "    # Render the environment to the screen\n",
    "    def render(self, date, open, high, low, close, volume, net_worth, trades):\n",
    "        # append volume and net_worth to deque list\n",
    "        self.volume.append(volume)\n",
    "        self.net_worth.append(net_worth)\n",
    "\n",
    "        # before appending to deque list, need to convert Date to special format\n",
    "        date = mpl_dates.date2num([pd.to_datetime(date)])[0]\n",
    "        self.render_data.append([date, open, high, low, close])\n",
    "        \n",
    "        # Clear the frame rendered last step\n",
    "        self.ax1.clear()\n",
    "        candlestick_ohlc(self.ax1, self.render_data, width=0.8/24, colorup='red', colordown='blue', alpha=0.8)\n",
    "\n",
    "        # Put all dates to one list and fill ax2 sublot with volume\n",
    "        date_render_range = [i[0] for i in self.render_data]\n",
    "        self.ax2.clear()\n",
    "        self.ax2.fill_between(date_render_range, self.volume, 0)\n",
    "\n",
    "        # draw our net_worth graph on ax3 (shared with ax1) subplot\n",
    "        self.ax3.clear()\n",
    "        self.ax3.plot(date_render_range, self.net_worth, color=\"blue\")\n",
    "        \n",
    "        # beautify the x-labels (Our Date format)\n",
    "        self.ax1.xaxis.set_major_formatter(self.date_format)\n",
    "        self.fig.autofmt_xdate()\n",
    "\n",
    "        # sort sell and buy orders, put arrows in appropiate order positions\n",
    "        for trade in trades:\n",
    "            trade_date = mpl_dates.date2num([pd.to_datetime(trade['Date'])])[0]\n",
    "            if trade_date in date_render_range:\n",
    "                if trade['Type'] == 'buy':\n",
    "                    high_low = trade['Low']-10\n",
    "                    self.ax1.scatter(trade_date, high_low, c='red', label='red', s = 120, edgecolors='none', marker=\"^\")\n",
    "                else:\n",
    "                    high_low = trade['High']+10\n",
    "                    self.ax1.scatter(trade_date, high_low, c='blue', label='blue', s = 120, edgecolors='none', marker=\"v\")\n",
    "\n",
    "        # we need to set layers every step, because we are clearing subplots every step\n",
    "        self.ax2.set_xlabel('Date')\n",
    "        self.ax1.set_ylabel('Price')\n",
    "        self.ax3.yaxis.set_label_position('right')\n",
    "        self.ax3.set_ylabel('Balance') # 여기 수정\n",
    "\n",
    "        # I use tight_layout to replace plt.subplots_adjust\n",
    "        self.fig.tight_layout()\n",
    "\n",
    "        \"\"\"Display image with matplotlib - interrupting other tasks\"\"\"\n",
    "        # Show the graph without blocking the rest of the program\n",
    "        plt.show(block=False)\n",
    "        # Necessary to view frames before they are unrendered\n",
    "        plt.pause(0.001)\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "        self.Transition = namedtuple('Transition',\n",
    "                                ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(self.Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# Weight initialisation\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "#def OU_noise(action, mu=0, theta=0.15, sigma=0.3):\n",
    "#    dx = -theta\n",
    "\n",
    "#class OU_noise: # state에 노이즈를 입히는 것이 아니라 action에 노이즈를 입히는 것이다.\n",
    "#    def __init__(self, state, mu=0, theta=0.15, sigma=0.3):\n",
    "#        self.state = state\n",
    "#        self.n_actions = 1\n",
    "#        self.mu = mu\n",
    "#        self.theta = theta\n",
    "#        self.sigma = sigma\n",
    "#        self.state = np.ones(self.n_actions) * self.mu\n",
    "#        self.reset()\n",
    "#\n",
    "#    def reset(self):\n",
    "#        self.state = np.ones(self.n_actions) * self.mu\n",
    "#\n",
    "#    def sample(self):\n",
    "#        x = self.state\n",
    "#        dx = self.beta * (self.alpha - x) + self.sigma * np.random.randn(len(x))\n",
    "#        self.state = x + dx\n",
    "#        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Actor, self).__init__()\n",
    "        self.n_actions = 1\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layer1 = nn.Linear(500, 512)\n",
    "        self.layer2 = nn.Linear(512, 256)\n",
    "        self.layer3 = nn.Linear(256, 64)\n",
    "        self.layer4 = nn.Linear(64, self.n_actions, bias=False) # continuos action space니까 1이 되어야 하지 않을까? # \n",
    "        # action size: 여기서는 비트코인 한 종류만 다루고 있으므로 마지막을 1로 처리\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = state\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.relu(self.layer3(x)) # 뒤에 tanh가 오는데 ReLU를 또 써?\n",
    "        #x = self.layer3(x)\n",
    "\n",
    "        return torch.tanh(self.layer4(x)) # tanh 함수는 결과값을 -1에서 1 사이로 가두어 준다. 그러므로 continuous action space에 알맞다.\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        self.n_actions = 1\n",
    "        # Layer 1\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layer1 = nn.Linear(500, 512)\n",
    "        self.batch_norm_1 = nn.BatchNorm1d(512)\n",
    "\n",
    "        # Layer 2\n",
    "        # In the second layer the actions will be inserted also \n",
    "        self.layer2 = nn.Linear(512, 256)\n",
    "        self.batch_norm_2 = nn.BatchNorm1d(256)\n",
    "\n",
    "        self.layer3 = nn.Linear(256 + self.n_actions, 64)\n",
    "        self.batch_norm_3 = nn.BatchNorm1d(64)\n",
    "\n",
    "        # Output layer (single value)\n",
    "        self.layer4 = nn.Linear(64, 1)\n",
    "\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = state\n",
    "\n",
    "        # Layer 1\n",
    "        x = self.flatten(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.batch_norm_1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Layer 2\n",
    "        x = self.layer2(x)\n",
    "        x = self.batch_norm_2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = torch.cat((x, action), 1)  # Insert the actions\n",
    "        x = self.layer3(x)\n",
    "        x = self.batch_norm_3(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        return self.layer4(x) # Value라서 x 대신 V라고도 부른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "#from tensorboardX import SummaryWriter\n",
    "import gymnasium as gym\n",
    "\n",
    "from utils_ddpg import TradingGraph, ReplayMemory, init_weights #, OU_noise\n",
    "from model import Actor, Critic\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "\n",
    "class TradingEnv:\n",
    "    # A custom Bitcoin trading environment\n",
    "    def __init__(self, df, initial_balance=1000, lookback_window_size=50, render_range = 100):\n",
    "        # Define action space and state size and other custom parameters\n",
    "        self.df = df.dropna().reset_index()\n",
    "        self.df_total_steps = len(self.df)-1\n",
    "        self.initial_balance = initial_balance\n",
    "        self.lookback_window_size = lookback_window_size\n",
    "        self.render_range = render_range # render range in visualization\n",
    "\n",
    "        self.BATCH_SIZE = 64\n",
    "        self.GAMMA = 0.99\n",
    "        self.TAU = 0.1\n",
    "        self.LR = 1e-4\n",
    "        self.actor_loss = 0\n",
    "        self.critic_loss = 0\n",
    "        self.actor_losses = []\n",
    "        self.critic_losses = []\n",
    "        self.C = 24 * 30 # 한달마다 거래전략 수정\n",
    "\n",
    "        #self.OU = OU_noise(1)\n",
    "\n",
    "    \n",
    "        self.memory = ReplayMemory(1000)\n",
    "\n",
    "        # Action space from -1 to 1. -1\n",
    "        self.action_space = gym.spaces.Box(low=-1, high=1, shape=(1,)) \n",
    "\n",
    "        # Orders history contains the balance, net_worth, crypto_bought, crypto_sold, crypto_held values for the last lookback_window_size steps\n",
    "        self.orders_history = deque(maxlen=self.lookback_window_size)\n",
    "        \n",
    "        # Market history contains the OHCL values for the last lookback_window_size prices\n",
    "        self.market_history = deque(maxlen=self.lookback_window_size)\n",
    "\n",
    "        # State size contains Market+Orders history for the last lookback_window_size steps\n",
    "        self.state_size = (self.lookback_window_size, 10)\n",
    "\n",
    "        self.Actor_target = Actor().to(device)\n",
    "        self.Actor_target.apply(init_weights)\n",
    "        self.Actor_behaviour = copy.deepcopy(self.Actor_target).to(device)\n",
    "        self.Critic_target = Critic().to(device)\n",
    "        self.Critic_target.apply(init_weights)\n",
    "        self.Critic_behaviour = copy.deepcopy(self.Critic_target).to(device)\n",
    "\n",
    "\n",
    "        self.actor_optimizer = optim.Adam(self.Actor_behaviour.parameters(), lr=self.LR)\n",
    "        self.critic_optimizer = optim.Adam(self.Critic_behaviour.parameters(), lr=self.LR)\n",
    "\n",
    "    # Create tensorboard writer\n",
    "#    def create_writer(self):\n",
    "#        self.replay_count = 0\n",
    "#        self.writer = SummaryWriter(comment=\"Crypto_trader\")\n",
    "\n",
    "    # Reset the state of the environment to an initial state\n",
    "    def reset(self, env_steps_size = 0):\n",
    "        self.visualization = TradingGraph(render_range=self.render_range) # init visualization\n",
    "        self.trades = deque(maxlen=self.render_range) # limited orders memory for visualization\n",
    "        \n",
    "        self.balance = self.initial_balance\n",
    "        self.net_worth = self.initial_balance\n",
    "        self.prev_net_worth = self.initial_balance\n",
    "        self.crypto_held = 0\n",
    "        self.crypto_sold = 0\n",
    "        self.crypto_bought = 0\n",
    "\n",
    "        if env_steps_size > 0: # used for training dataset\n",
    "            self.start_step = random.randint(self.lookback_window_size, self.df_total_steps - env_steps_size)\n",
    "            self.end_step = self.start_step + env_steps_size\n",
    "        else: # used for testing dataset\n",
    "            self.start_step = self.lookback_window_size\n",
    "            self.end_step = self.df_total_steps\n",
    "            \n",
    "        self.current_step = self.start_step\n",
    "\n",
    "        for i in reversed(range(self.lookback_window_size)):\n",
    "            current_step = self.current_step - i\n",
    "            self.orders_history.append([self.balance, self.net_worth, self.crypto_bought, self.crypto_sold, self.crypto_held])\n",
    "            self.market_history.append([self.df.loc[current_step, 'open'],\n",
    "                                        self.df.loc[current_step, 'high'],\n",
    "                                        self.df.loc[current_step, 'low'],\n",
    "                                        self.df.loc[current_step, 'close'],\n",
    "                                        self.df.loc[current_step, 'volume']\n",
    "                                        ])\n",
    "\n",
    "        state = np.concatenate((self.market_history, self.orders_history), axis=1)\n",
    "        return state\n",
    "\n",
    "    # Get the data points for the given current_step\n",
    "    def _next_observation(self):\n",
    "        self.market_history.append([self.df.loc[self.current_step, 'open'],\n",
    "                                    self.df.loc[self.current_step, 'high'],\n",
    "                                    self.df.loc[self.current_step, 'low'],\n",
    "                                    self.df.loc[self.current_step, 'close'],\n",
    "                                    self.df.loc[self.current_step, 'volume']\n",
    "                                    ])\n",
    "        obs = np.concatenate((self.market_history, self.orders_history), axis=1)\n",
    "\n",
    "        return obs\n",
    "\n",
    "    # Execute one time step within the environment\n",
    "    def step(self, action):\n",
    "        #action = action.cpu()\n",
    "        if action == int(action):\n",
    "            pass\n",
    "        else:\n",
    "            action = action.item()\n",
    "\n",
    "        #action = action.cpu() # 새로 추가한 부분\n",
    "        self.crypto_bought = 0\n",
    "        self.crypto_sold = 0\n",
    "        self.current_step += 1\n",
    "\n",
    "        # Set the current price to a random price between open and close\n",
    "        current_price = random.uniform(\n",
    "            self.df.loc[self.current_step, 'open'],\n",
    "            self.df.loc[self.current_step, 'close'])\n",
    "        date = self.df.loc[self.current_step, 'date_open'] # for visualization\n",
    "        high = self.df.loc[self.current_step, 'high'] # for visualization\n",
    "        low = self.df.loc[self.current_step, 'low'] # for visualization\n",
    "        \n",
    "        if action == 0: # Hold\n",
    "            pass\n",
    "        \n",
    "        elif action > 0 and self.balance > 0:#self.initial_balance/100: # 0이 아닌 이유: 가끔 오류가 날 때 있음\n",
    "            self.crypto_bought = self.balance * action / current_price # 현금 * action으로 정해진 비율만큼 이용해서 구매\n",
    "            self.balance -= self.crypto_bought * current_price\n",
    "            self.crypto_held += self.crypto_bought\n",
    "            self.trades.append({'Date' : date, 'High' : high, 'Low' : low, 'Total': self.crypto_bought, 'Type': \"buy\"})\n",
    "        \n",
    "        elif action < 0 and self.crypto_held * abs(action) > 0: \n",
    "            self.crypto_sold = self.crypto_held * abs(action) \n",
    "            self.balance += self.crypto_sold * abs(action) * current_price\n",
    "            self.crypto_held -= self.crypto_sold\n",
    "            self.trades.append({'Date' : date, 'High' : high, 'Low' : low, 'Total': self.crypto_sold, 'Type': \"sell\"})\n",
    "\n",
    "        self.prev_net_worth = self.net_worth\n",
    "        self.net_worth = self.balance + self.crypto_held * current_price\n",
    "\n",
    "        self.orders_history.append([self.balance, self.net_worth, self.crypto_bought, self.crypto_sold, self.crypto_held])\n",
    "\n",
    "        # Calculate reward\n",
    "        reward = self.net_worth - self.prev_net_worth\n",
    "\n",
    "        if self.net_worth <= self.initial_balance/2:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        obs = self._next_observation()\n",
    "        #obs = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "        return obs, reward, done\n",
    "\n",
    "    # render environment\n",
    "    def render(self, visualize=False):\n",
    "        #print(f'Step: {self.current_step}, Net Worth: {self.net_worth}')\n",
    "        if visualize:\n",
    "            date = self.df.loc[self.current_step, 'date_open']\n",
    "            open = self.df.loc[self.current_step, 'open']\n",
    "            close = self.df.loc[self.current_step, 'close']\n",
    "            high = self.df.loc[self.current_step, 'high']\n",
    "            low = self.df.loc[self.current_step, 'low']\n",
    "            volume = self.df.loc[self.current_step, 'volume']\n",
    "\n",
    "            # Render the environment to the screen\n",
    "            self.visualization.render(date, open, high, low, close, volume, self.net_worth, self.trades)\n",
    "    \n",
    "    def act(self, state, testmode): # select_action에 대응\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            action = self.Actor_behaviour(state) \n",
    "\n",
    "        #action = action.cpu()\n",
    "        #action = action.item()\n",
    "        action_with_noise = action + torch.rand(1).to(device=device) * 0.1\n",
    "\n",
    "        if action_with_noise > 1:\n",
    "            action_with_noise = 1\n",
    "        elif action_with_noise < -1:\n",
    "            action_with_noise = -1\n",
    "        \n",
    "        if testmode == False:\n",
    "            #return torch.tensor([[action_with_noise]], device=device, dtype=torch.long)\n",
    "            return action_with_noise\n",
    "        else:\n",
    "            #return torch.tensor([[action]], device=device, dtype=torch.long)\n",
    "            return action\n",
    "\n",
    "\n",
    "        #if sample > eps_threshold:\n",
    "        #    with torch.no_grad():\n",
    "        #        return self.Actor(state).max(1).indices.view(1,1)\n",
    "        #else:\n",
    "        #    return torch.tensor([[np.random.choice(self.action_space)]], device=device, dtype=torch.long)\n",
    "    \n",
    "\n",
    "    def save(self, name=\"ddpg\"):\n",
    "        torch.save(self.Actor_target.state_dict(), f'./ddpg/{name}_Actor.h5')\n",
    "        torch.save(self.Critic_target.state_dict(), f'./ddpg/{name}_Critic.h5')\n",
    "\n",
    "    def load(self, name=\"ddpg\"):\n",
    "        self.Actor_target.load_state_dict(torch.load(f'./ddpg/{name}_Actor.h5', weights_only=True))\n",
    "        self.Critic_target.load_state_dict(torch.load(f'./ddpg/{name}_Critic.h5', weights_only=True))\n",
    "        \n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < self.BATCH_SIZE * 5:\n",
    "            return\n",
    "        Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "        transitions = self.memory.sample(self.BATCH_SIZE)\n",
    "        # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "        # detailed explanation). This converts batch-array of Transitions\n",
    "        # to Transition of batch-arrays.\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # Compute a mask of non-final states and concatenate the batch elements\n",
    "        # (a final state would've been the one after which simulation ended)\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "        \n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        #action_batch = action_batch.squeeze(1)\n",
    "\n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "        # columns of actions taken. These are the actions which would've been taken\n",
    "        # for each batch state according to policy_net\n",
    "        #state_action_values = self.Actor(state_batch).gather(1, action_batch)\n",
    "        #state_action_values = self.Actor(state_batch).gather(1, action_batch) \n",
    "        state_action_values = self.Critic_target(state_batch, action_batch)\n",
    "\n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        # Expected values of actions for non_final_next_states are computed based\n",
    "        # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "        # This is merged based on the mask, such that we'll have either the expected\n",
    "        # state value or 0 in case the state was final.\n",
    "        next_state_values = torch.zeros(self.BATCH_SIZE, device=device)\n",
    "        #with torch.no_grad():\n",
    "        #    #next_state_values[non_final_mask] = self.Actor(non_final_next_states, state_action_values).max(1).values\n",
    "        #    next_state_values = self.Critic_behaviour(non_final_next_states, self.Actor_behaviour(non_final_next_states))#.max(1).values\n",
    "        #    next_state_values = next_state_values.squeeze(1)\n",
    "        \n",
    "        next_state_values = next_state_values.unsqueeze(1)\n",
    "        next_state_values[non_final_mask] = self.Critic_behaviour(non_final_next_states, self.Actor_behaviour(non_final_next_states))#.max(1).values\n",
    "        next_state_values = next_state_values.squeeze()\n",
    "\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = (next_state_values * self.GAMMA) + reward_batch\n",
    "        expected_state_action_values = expected_state_action_values.unsqueeze(1)\n",
    "\n",
    "\n",
    "        self.Critic_behaviour.train()\n",
    "        critic_criterion = nn.HuberLoss()\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        self.critic_loss = critic_criterion(state_action_values, expected_state_action_values)\n",
    "        #torch.nn.utils.clip_grad_value_(self.Critic_behaviour.parameters(), 100)\n",
    "        self.critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        self.Actor_behaviour.train()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        self.actor_loss = -torch.mean(self.Critic_behaviour(state_batch, self.Actor_behaviour(state_batch)))\n",
    "        #torch.nn.utils.clip_grad_value_(self.Actor_behaviour.parameters(), 100)\n",
    "        self.actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        if self.current_step % self.C == 0:\n",
    "            self.soft_update_target(self.Critic_target, self.Critic_behaviour)\n",
    "            self.soft_update_target(self.Actor_target, self.Actor_behaviour)\n",
    "\n",
    "    def soft_update_target(self, target, original):\n",
    "        for target_parameters, original_parameters in zip(target.parameters(), original.parameters()):\n",
    "            target_parameters.data.copy_((1 - self.TAU) * target_parameters.data + self.TAU * original_parameters.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_agent(env, visualize=False, train_episodes=1, training_batch_size=500):\n",
    "    #env.create_writer() # create TensorBoard writer\n",
    "    #total_average = deque(maxlen=100) # save recent 100 episodes net worth\n",
    "    #best_average = 0 # used to track best average net worth\n",
    "    \n",
    "    for episode in range(train_episodes):\n",
    "        state = env.reset(env_steps_size = training_batch_size)\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "        for t in range(training_batch_size):\n",
    "            env.render(visualize)\n",
    "            action = env.act(state, testmode=False)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            next_state = torch.tensor(next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            reward = torch.tensor(reward, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            #action = torch.tensor(action, dtype=torch.float32, device=device).unsqueeze(0).unsqueeze(0)\n",
    "            if done:\n",
    "                break\n",
    "            #action_onehot = np.zeros(3)\n",
    "            #action_onehot[action] = 1\n",
    "            env.memory.push(state, action, next_state, reward) # Store the transition in memory\n",
    "            #next_state = torch.tensor(next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            state = next_state\n",
    "            env.optimize_model() # perform one step of the optimization on the policy network\n",
    "\n",
    "            print(f\"Episode {episode} net_worth: {env.net_worth}, step: {env.current_step}\")\n",
    "            print(f\"Episode {episode} Actor loss: {env.actor_loss} Critic loss: {env.critic_loss}\")\n",
    "        if episode == train_episodes - 1:\n",
    "            env.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(env, visualize=False, test_episodes=1):\n",
    "    env.load() # load the model\n",
    "    for episode in range(test_episodes):\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            env.render(visualize)\n",
    "            action = env.act(state, testmode=True)\n",
    "            state, reward, done = env.step(action)\n",
    "            print(f\"Episode {episode} net_worth: {env.net_worth}\")\n",
    "            if env.current_step == env.end_step:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    jk = train_env.Actor_behaviour(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05337198])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(1)*0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = torch.tensor(5).unsqueeze(0).unsqueeze(0)\n",
    "test2 = torch.tensor(3).unsqueeze(0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5]])"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3]])"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8]])"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1 + test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sum() received an invalid combination of arguments - got (Tensor, Tensor, dtype=torch.dtype), but expected one of:\n * (Tensor input, *, torch.dtype dtype)\n      didn't match because some of the arguments have invalid types: (Tensor, !Tensor!, dtype=torch.dtype)\n * (Tensor input, tuple of ints dim, bool keepdim, *, torch.dtype dtype, Tensor out)\n * (Tensor input, tuple of names dim, bool keepdim, *, torch.dtype dtype, Tensor out)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[303], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: sum() received an invalid combination of arguments - got (Tensor, Tensor, dtype=torch.dtype), but expected one of:\n * (Tensor input, *, torch.dtype dtype)\n      didn't match because some of the arguments have invalid types: (Tensor, !Tensor!, dtype=torch.dtype)\n * (Tensor input, tuple of ints dim, bool keepdim, *, torch.dtype dtype, Tensor out)\n * (Tensor input, tuple of names dim, bool keepdim, *, torch.dtype dtype, Tensor out)\n"
     ]
    }
   ],
   "source": [
    "torch.sum(test1, test2, dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11151, 6)"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26016, 6)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.4634])"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.normal(mean=0, std=1, size=(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4w/q37s19t15xg3s29s2zm_1dcw0000gn/T/ipykernel_77536/3396839587.py:188: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 net_worth: 1000.0, step: 969\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 992.3905224792661, step: 970\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 996.1121183226408, step: 971\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 997.5434901059837, step: 972\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 984.1576590450336, step: 973\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 964.6383336409896, step: 974\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 943.1030697376854, step: 975\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 966.0015173766221, step: 976\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 965.6585980962172, step: 977\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 977.4561408403273, step: 978\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 987.2420367950224, step: 979\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 993.1117921909689, step: 980\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 995.6496456024175, step: 981\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 996.4243253950226, step: 982\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 996.0592349717855, step: 983\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 992.7720966485629, step: 984\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 993.9545729149794, step: 985\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 982.7144740698855, step: 986\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 987.0321294687541, step: 987\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 987.8678950110061, step: 988\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 985.9002521748935, step: 989\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 982.4369511969004, step: 990\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 976.9309885222337, step: 991\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 968.1187101748518, step: 992\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 921.0498217494286, step: 993\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 921.3346683863591, step: 994\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 915.7556274129304, step: 995\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 917.5853026761043, step: 996\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 918.999835904137, step: 997\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 899.5462448641983, step: 998\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 883.4612721101093, step: 999\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 889.1789635786671, step: 1000\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 901.7165166006139, step: 1001\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 894.4631149144682, step: 1002\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 892.379701971203, step: 1003\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 888.9100807521434, step: 1004\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 886.2929605854508, step: 1005\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 890.7505564319715, step: 1006\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 891.0185925475646, step: 1007\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 893.3098929566634, step: 1008\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 897.2733333135407, step: 1009\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 901.8379029520321, step: 1010\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 903.0033001306128, step: 1011\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 902.9292988166106, step: 1012\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 897.3633770077989, step: 1013\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 904.561720188256, step: 1014\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 906.8377808934495, step: 1015\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 908.7338722764115, step: 1016\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 909.2266154590852, step: 1017\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 908.559569567615, step: 1018\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 905.6682302215638, step: 1019\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 908.7958575911172, step: 1020\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 918.7607807330642, step: 1021\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 921.1174776023173, step: 1022\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 920.0670739520561, step: 1023\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 918.9544003761695, step: 1024\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 911.9474134481899, step: 1025\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 911.6248426810556, step: 1026\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 908.6030795058962, step: 1027\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 909.6319478708957, step: 1028\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 911.7143513322852, step: 1029\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 916.5118100734043, step: 1030\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 924.5849074248603, step: 1031\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 924.732803309121, step: 1032\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 923.2060245552908, step: 1033\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 922.5214144957979, step: 1034\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 920.0601889447358, step: 1035\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 919.8254541903016, step: 1036\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 920.6362659826214, step: 1037\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 921.6612093332448, step: 1038\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 922.0673736634741, step: 1039\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 922.9809374769474, step: 1040\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 933.154742341838, step: 1041\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 936.1372189732712, step: 1042\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 939.594401565106, step: 1043\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 939.6351215466439, step: 1044\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 941.2064881761228, step: 1045\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 942.8491518386498, step: 1046\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 942.1122609250066, step: 1047\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 939.3118425879702, step: 1048\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 940.9151261018642, step: 1049\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 953.7932224594922, step: 1050\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 963.2539403365862, step: 1051\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 966.3040377378238, step: 1052\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 963.8332979544922, step: 1053\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 963.1631072524751, step: 1054\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 960.1193327202438, step: 1055\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 960.24233085973, step: 1056\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 966.9510272566237, step: 1057\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 967.5374670365393, step: 1058\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 964.6070912066677, step: 1059\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 976.8398391695248, step: 1060\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 977.2579304233141, step: 1061\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 989.1962274844242, step: 1062\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1005.5633227729628, step: 1063\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1010.6789570676805, step: 1064\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1000.7895244986344, step: 1065\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1001.162210828511, step: 1066\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 993.497012239997, step: 1067\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 996.349874092258, step: 1068\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1008.0701051997657, step: 1069\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1008.1961258581302, step: 1070\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1000.4836655876976, step: 1071\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 996.8727847379616, step: 1072\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 999.5258931340145, step: 1073\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 999.3164656749036, step: 1074\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1000.9049610456576, step: 1075\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1010.9729050017646, step: 1076\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1012.1061657756071, step: 1077\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1012.8739989622816, step: 1078\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1008.5972495395534, step: 1079\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 998.62356692482, step: 1080\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 980.3262240821723, step: 1081\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 977.5583011766462, step: 1082\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 979.5400758063155, step: 1083\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 986.3687796298452, step: 1084\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 995.2168491294224, step: 1085\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 999.5250911952154, step: 1086\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 994.5997888885514, step: 1087\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 992.844291012097, step: 1088\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 992.2916132685923, step: 1089\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 987.2862773219068, step: 1090\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 985.9332929182508, step: 1091\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 988.3778197113378, step: 1092\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 986.7036537252167, step: 1093\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 980.073280158253, step: 1094\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 980.0645073982156, step: 1095\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 981.5024772824838, step: 1096\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 969.9666072331994, step: 1097\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 958.4351920614729, step: 1098\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 972.2776875657775, step: 1099\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 970.6488453405699, step: 1100\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 967.4829471671826, step: 1101\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 966.2866468873924, step: 1102\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 972.3340814378473, step: 1103\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 973.909185085093, step: 1104\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 975.2886146435666, step: 1105\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 979.754614947404, step: 1106\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 980.802883701923, step: 1107\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 979.4944789039338, step: 1108\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 974.4699206824864, step: 1109\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 972.05748307992, step: 1110\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 975.0096927615335, step: 1111\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 974.4757455847479, step: 1112\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 976.5266893515918, step: 1113\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 975.1292709864716, step: 1114\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 978.2263538061654, step: 1115\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 977.6183803522097, step: 1116\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 970.9426827372039, step: 1117\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 970.9275554164843, step: 1118\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 965.937977570184, step: 1119\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 963.9651215639936, step: 1120\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 963.6908569367531, step: 1121\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 965.6732680999404, step: 1122\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 970.8127990861841, step: 1123\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 973.2341482690937, step: 1124\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 971.9827662115946, step: 1125\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 974.9429456821092, step: 1126\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 980.4869663787216, step: 1127\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 982.416890149719, step: 1128\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 985.2094508705778, step: 1129\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 984.6156159383528, step: 1130\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 982.6321742250339, step: 1131\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 986.4558426498395, step: 1132\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 987.9157130369525, step: 1133\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 982.34767284675, step: 1134\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 982.0904981545194, step: 1135\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 983.6294016989773, step: 1136\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 985.8033132886795, step: 1137\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 998.4892514085145, step: 1138\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1010.7419311031132, step: 1139\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1011.647998203664, step: 1140\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1015.2091932873184, step: 1141\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1011.6449938689059, step: 1142\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1008.8559057955432, step: 1143\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1009.0386344269931, step: 1144\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1010.8001468619003, step: 1145\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1010.0322472116652, step: 1146\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1009.8950493839876, step: 1147\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1004.502784281866, step: 1148\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1001.9507057787804, step: 1149\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1008.9520918539804, step: 1150\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1022.2117717708526, step: 1151\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1022.6737691358913, step: 1152\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1020.9645010685475, step: 1153\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1020.7344767264404, step: 1154\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1020.5409963454204, step: 1155\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1015.0934150867279, step: 1156\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1012.4209601569422, step: 1157\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1000.2022373439164, step: 1158\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 992.2900737296376, step: 1159\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 992.2456436994293, step: 1160\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 996.5539855074062, step: 1161\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 998.6191435375903, step: 1162\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1000.0334864281192, step: 1163\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1003.674359936067, step: 1164\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1004.0581206203451, step: 1165\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 997.7774099923859, step: 1166\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 997.0733642418704, step: 1167\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 998.4266574262423, step: 1168\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1001.9151537557939, step: 1169\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1002.2543695390067, step: 1170\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1002.2038224715648, step: 1171\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1004.8820893235314, step: 1172\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1006.7122004919796, step: 1173\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 998.4853902012275, step: 1174\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 998.3987622086479, step: 1175\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 996.4945186111808, step: 1176\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 989.0743659453767, step: 1177\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 986.213630178928, step: 1178\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 992.8753536365857, step: 1179\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 992.2205162865699, step: 1180\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 990.6290094533001, step: 1181\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 993.1413279007642, step: 1182\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1013.4175256410041, step: 1183\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1013.1819881819847, step: 1184\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1009.997169594317, step: 1185\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1000.0429008401807, step: 1186\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 996.4147150530815, step: 1187\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1002.4057522989771, step: 1188\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1003.28635235823, step: 1189\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1002.8607979509234, step: 1190\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1001.5397978626244, step: 1191\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 999.5225226779693, step: 1192\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 998.158173017546, step: 1193\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1002.7234540278853, step: 1194\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1002.6341045810756, step: 1195\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1002.9831447911259, step: 1196\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1010.0638576151965, step: 1197\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1010.9962260778271, step: 1198\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1008.0260845240236, step: 1199\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1007.5153720505858, step: 1200\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1007.0566948230647, step: 1201\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1006.7168987960815, step: 1202\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1007.684403862907, step: 1203\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1006.5590437574274, step: 1204\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1008.7664603709486, step: 1205\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1011.6185695099383, step: 1206\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1012.6920967653491, step: 1207\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1011.7932785854324, step: 1208\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1010.6641890755762, step: 1209\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1010.0277194020133, step: 1210\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1012.8106111443348, step: 1211\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 1011.2496267325536, step: 1212\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 993.819448417911, step: 1213\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 978.9719916886045, step: 1214\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 986.509873906765, step: 1215\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 986.504060800355, step: 1216\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 986.2749003118083, step: 1217\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 991.3956120729471, step: 1218\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 989.8360053400841, step: 1219\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 987.4662378393439, step: 1220\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 985.4515337908022, step: 1221\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 988.2184457384469, step: 1222\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 985.6330068172956, step: 1223\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 980.9471290891606, step: 1224\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 981.1764242893564, step: 1225\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 981.2523757706425, step: 1226\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 982.8574555717807, step: 1227\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 981.8806518023029, step: 1228\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 981.6917864575356, step: 1229\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 973.7390383252924, step: 1230\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 969.095750718401, step: 1231\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 964.6204869869539, step: 1232\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 971.4526590395376, step: 1233\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 970.7814736545176, step: 1234\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 962.0457276448691, step: 1235\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 947.0826849850679, step: 1236\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 939.9540283459553, step: 1237\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 939.6712226263462, step: 1238\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 933.0906002034797, step: 1239\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 928.0740423949908, step: 1240\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 931.0180019806667, step: 1241\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 937.581694303258, step: 1242\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 939.0319332735436, step: 1243\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 944.0377677195636, step: 1244\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 940.6957901602254, step: 1245\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 934.4396915982283, step: 1246\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 936.2407460525745, step: 1247\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 940.506586960549, step: 1248\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 931.3716766325066, step: 1249\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 930.9924158024044, step: 1250\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 932.104721612959, step: 1251\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 940.0910154202608, step: 1252\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 940.0530090261019, step: 1253\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 940.4711189447814, step: 1254\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 942.2504726871754, step: 1255\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 945.5191906120292, step: 1256\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 948.3274697418327, step: 1257\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 949.0836527301421, step: 1258\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 948.8615302348644, step: 1259\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 945.1561470604165, step: 1260\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 945.8653226256259, step: 1261\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 954.3392276960922, step: 1262\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 954.4306770373577, step: 1263\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 951.3284727494113, step: 1264\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 950.399207005894, step: 1265\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 949.3449572506021, step: 1266\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 952.1439730177891, step: 1267\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 952.4715806003766, step: 1268\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 951.193020269338, step: 1269\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 953.1774852029048, step: 1270\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 956.0319023231579, step: 1271\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 957.9463832680281, step: 1272\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 959.6370164164352, step: 1273\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 957.552573270424, step: 1274\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 958.4137035770748, step: 1275\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 956.6418214484895, step: 1276\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 957.0764519659242, step: 1277\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 953.9134348186086, step: 1278\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 947.8087867877549, step: 1279\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 945.6056004470445, step: 1280\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 947.8425055815239, step: 1281\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 944.9257917026724, step: 1282\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 945.4081152378593, step: 1283\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 948.9155840258449, step: 1284\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 951.7879696833294, step: 1285\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 950.0305664749998, step: 1286\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n",
      "Episode 0 net_worth: 949.4260701038052, step: 1287\n",
      "Episode 0 Actor loss: 0 Critic loss: 0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[325], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m train_df \u001b[38;5;241m=\u001b[39m df[:train_size]\n\u001b[1;32m      7\u001b[0m train_env \u001b[38;5;241m=\u001b[39m TradingEnv(train_df, lookback_window_size\u001b[38;5;241m=\u001b[39mlookback_window_size)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[323], line 24\u001b[0m, in \u001b[0;36mtrain_agent\u001b[0;34m(env, visualize, train_episodes, training_batch_size)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#next_state = torch.tensor(next_state, dtype=torch.float32, device=device).unsqueeze(0)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m---> 24\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# perform one step of the optimization on the policy network\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m net_worth: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv\u001b[38;5;241m.\u001b[39mnet_worth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, step: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv\u001b[38;5;241m.\u001b[39mcurrent_step\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Actor loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv\u001b[38;5;241m.\u001b[39mactor_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Critic loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv\u001b[38;5;241m.\u001b[39mcritic_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[231], line 241\u001b[0m, in \u001b[0;36mTradingEnv.optimize_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    238\u001b[0m non_final_next_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([s \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mnext_state \u001b[38;5;28;01mif\u001b[39;00m s \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m])\n\u001b[1;32m    240\u001b[0m state_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(batch\u001b[38;5;241m.\u001b[39mstate)\n\u001b[0;32m--> 241\u001b[0m action_batch \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m reward_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(batch\u001b[38;5;241m.\u001b[39mreward)\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m#action_batch = action_batch.squeeze(1)\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \n\u001b[1;32m    245\u001b[0m \u001b[38;5;66;03m# Compute Q(s_t, a) - the model computes Q(s_t), then we select the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m#state_action_values = self.Actor(state_batch).gather(1, action_batch)\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m#state_action_values = self.Actor(state_batch).gather(1, action_batch) \u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got int"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x800 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lookback_window_size = 50\n",
    "data_path = \"../data/binance-BTCUSDT-1h.pkl\"\n",
    "df = pd.read_pickle(data_path)\n",
    "train_size = int(len(df) * 0.7)\n",
    "train_df = df[:train_size]\n",
    "\n",
    "train_env = TradingEnv(train_df, lookback_window_size=lookback_window_size)\n",
    "\n",
    "train_agent(train_env, train_episodes=2, training_batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwer = torch.tensor(5)\n",
    "qwer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(qwer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4w/q37s19t15xg3s29s2zm_1dcw0000gn/T/ipykernel_77536/1927626417.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  qwer = torch.tensor(qwer, dtype=torch.float32, device=device).unsqueeze(0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwer = torch.tensor(qwer, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "qwer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4w/q37s19t15xg3s29s2zm_1dcw0000gn/T/ipykernel_77536/3396839587.py:188: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x800 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 우선 100번 timestep만큼 돌려보자\n",
    "\n",
    "lookback_window_size = 50\n",
    "data_path = \"../data/binance-BTCUSDT-1h.pkl\"\n",
    "df = pd.read_pickle(data_path)\n",
    "train_size = int(len(df) * 0.7)\n",
    "train_df = df[:train_size]\n",
    "\n",
    "lookback_window_size = 50\n",
    "\n",
    "train_env = TradingEnv(train_df, lookback_window_size=lookback_window_size)\n",
    "\n",
    "memory = ReplayMemory(1000)\n",
    "\n",
    "state = train_env.reset(env_steps_size = 500)\n",
    "state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "for _ in range(100):\n",
    "    action = train_env.act(state, testmode=False)\n",
    "    next_state, reward, done = train_env.step(action)\n",
    "    next_state = torch.tensor(next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    reward = torch.tensor(reward, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    action = torch.tensor(action, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    memory.push(state, action, next_state, reward)\n",
    "    state = next_state\n",
    "    #train_env.optimize_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                ('state', 'action', 'next_state', 'reward'))\n",
    "transitions = memory.sample(5)\n",
    "batch = Transition(*zip(*transitions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                    batch.next_state)), device=device, dtype=torch.bool)\n",
    "#non_final_mask = non_final_mask.unsqueeze(1)\n",
    "non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                            if s is not None])\n",
    "state_batch = torch.cat(batch.state)\n",
    "action_batch = torch.cat(batch.action)\n",
    "reward_batch = torch.cat(batch.reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.], device='mps:0')"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_batch = action_batch.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1])"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]], device='mps:0')"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_action_values = train_env.Critic_target(state_batch, action_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14564.3311],\n",
       "        [14176.2676],\n",
       "        [14924.9541],\n",
       "        [13175.2119],\n",
       "        [14643.8428]], device='mps:0', grad_fn=<LinearBackward0>)"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_state_values = torch.zeros(5, device=device)\n",
    "next_state_values = next_state_values.unsqueeze(1)\n",
    "next_state_values[non_final_mask] = train_env.Critic_behaviour(non_final_next_states, train_env.Actor_behaviour(non_final_next_states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[16393.7109],\n",
       "        [15115.3018],\n",
       "        [14176.2676],\n",
       "        [13442.6133],\n",
       "        [14419.3613]], device='mps:0', grad_fn=<AsStridedBackward0>)"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1])"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_state_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5])"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_state_values = next_state_values.squeeze()\n",
    "next_state_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_state_action_values = (next_state_values * 0.99) + reward_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([16229.9170, 14966.0068, 14035.6611, 13308.0127, 14275.9980],\n",
       "       device='mps:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_state_action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1])"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_state_action_values = expected_state_action_values.unsqueeze(1)\n",
    "expected_state_action_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]], device='mps:0')"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_env.Critic_behaviour.train()\n",
    "critic_criterion = nn.HuberLoss()\n",
    "critic_optimizer = optim.Adam(train_env.Critic_behaviour.parameters(), lr=0.0001)\n",
    "train_env.Critic_behaviour.zero_grad()\n",
    "critic_loss = critic_criterion(state_action_values, expected_state_action_values)\n",
    "critic_loss.backward()\n",
    "torch.nn.utils.clip_grad_value_(train_env.Actor_behaviour.parameters(), 100)\n",
    "critic_optimizer.step()\n",
    "\n",
    "\n",
    "train_env.Actor_behaviour.train()\n",
    "actor_optimizer = optim.Adam(train_env.Actor_behaviour.parameters(), lr=0.0001)\n",
    "train_env.Actor_behaviour.zero_grad()\n",
    "actor_loss = -torch.mean(train_env.Critic_behaviour(state_batch, train_env.Actor_behaviour(state_batch)))\n",
    "actor_loss.backward()\n",
    "torch.nn.utils.clip_grad_value_(train_env.Critic_behaviour.parameters(), 100)\n",
    "actor_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-10171.0459, device='mps:0', grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(768.5527, device='mps:0', grad_fn=<HuberLossBackward0>)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1039.8612413446551"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_env.net_worth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.], device='mps:0')"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-cw-3-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
