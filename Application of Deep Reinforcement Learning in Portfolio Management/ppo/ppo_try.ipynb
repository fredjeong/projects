{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import deque\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.distributions import Normal\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mplfinance.original_flavor import candlestick_ohlc\n",
    "import matplotlib.dates as mpl_dates\n",
    "from datetime import datetime\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module): # action return\n",
    "    def __init__(self):\n",
    "        super(Actor, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layer1 = nn.Linear(500, 512) # state_dim = 500\n",
    "        self.layer2 = nn.Linear(512, 256)\n",
    "        self.layer3 = nn.Linear(256, 64)\n",
    "        self.mu_layer = nn.Linear(64, 1) # action_dim = 1\n",
    "        self.log_std_layer = nn.Linear(64, 1) # action_dim = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.relu(self.layer3(x))\n",
    "\n",
    "        mu = self.mu_layer(x)\n",
    "        log_std = torch.tanh(self.log_std_layer(x)) # -1에서 1 사이로 표준편차의 로그값 반환\n",
    "\n",
    "        return mu, log_std.exp()\n",
    "\n",
    "class Critic(nn.Module): # value return\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layer1 = nn.Linear(500, 512)\n",
    "        self.layer2 = nn.Linear(512, 256)\n",
    "        self.layer3 = nn.Linear(256, 64)\n",
    "        self.layer4 = nn.Linear(64, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.relu(self.layer3(x))\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutBuffer:\n",
    "    def __init__(self):\n",
    "        self.buffer = list()\n",
    "\n",
    "    def store(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample(self):\n",
    "        s, a, r, s_prime, done = map(np.array, zip(*self.buffer))\n",
    "        self.buffer.clear()\n",
    "        return (\n",
    "            torch.FloatTensor(s),\n",
    "            torch.FloatTensor(a),\n",
    "            torch.FloatTensor(r).unsqueeze(1),\n",
    "            torch.FloatTensor(s_prime),\n",
    "            torch.FloatTensor(done).unsqueeze(1)\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class TradingGraph:\n",
    "    # A crypto trading visualization using matplotlib made to render custom prices which come in following way:\n",
    "    # Date, Open, High, Low, Close, Volume, net_worth, trades\n",
    "    # call render every step\n",
    "    def __init__(self, render_range):\n",
    "        self.volume = deque(maxlen=render_range)\n",
    "        self.net_worth = deque(maxlen=render_range)\n",
    "        self.render_data = deque(maxlen=render_range)\n",
    "        self.render_range = render_range\n",
    "\n",
    "        # We are using the style ‘ggplot’\n",
    "        plt.style.use('ggplot')\n",
    "        # close all plots if there are open\n",
    "        plt.close('all')\n",
    "        # figsize attribute allows us to specify the width and height of a figure in unit inches\n",
    "        self.fig = plt.figure(figsize=(16,8)) \n",
    "\n",
    "        # Create top subplot for price axis\n",
    "        self.ax1 = plt.subplot2grid((6,1), (0,0), rowspan=5, colspan=1)\n",
    "        \n",
    "        # Create bottom subplot for volume which shares its x-axis\n",
    "        self.ax2 = plt.subplot2grid((6,1), (5,0), rowspan=1, colspan=1, sharex=self.ax1)\n",
    "        \n",
    "        # Create a new axis for net worth which shares its x-axis with price\n",
    "        self.ax3 = self.ax1.twinx()\n",
    "\n",
    "        # Formatting Date\n",
    "        self.date_format = mpl_dates.DateFormatter('%d-%m-%Y')\n",
    "        \n",
    "        # Add paddings to make graph easier to view\n",
    "        #plt.subplots_adjust(left=0.07, bottom=-0.1, right=0.93, top=0.97, wspace=0, hspace=0)\n",
    "\n",
    "    # Render the environment to the screen\n",
    "    def render(self, date, open, high, low, close, volume, net_worth, trades):\n",
    "        # append volume and net_worth to deque list\n",
    "        self.volume.append(volume)\n",
    "        self.net_worth.append(net_worth)\n",
    "\n",
    "        # before appending to deque list, need to convert Date to special format\n",
    "        date = mpl_dates.date2num([pd.to_datetime(date)])[0]\n",
    "        self.render_data.append([date, open, high, low, close])\n",
    "        \n",
    "        # Clear the frame rendered last step\n",
    "        self.ax1.clear()\n",
    "        candlestick_ohlc(self.ax1, self.render_data, width=0.8/24, colorup='red', colordown='blue', alpha=0.8)\n",
    "\n",
    "        # Put all dates to one list and fill ax2 sublot with volume\n",
    "        date_render_range = [i[0] for i in self.render_data]\n",
    "        self.ax2.clear()\n",
    "        self.ax2.fill_between(date_render_range, self.volume, 0)\n",
    "\n",
    "        # draw our net_worth graph on ax3 (shared with ax1) subplot\n",
    "        self.ax3.clear()\n",
    "        self.ax3.plot(date_render_range, self.net_worth, color=\"blue\")\n",
    "        \n",
    "        # beautify the x-labels (Our Date format)\n",
    "        self.ax1.xaxis.set_major_formatter(self.date_format)\n",
    "        self.fig.autofmt_xdate()\n",
    "\n",
    "        # sort sell and buy orders, put arrows in appropiate order positions\n",
    "        for trade in trades:\n",
    "            trade_date = mpl_dates.date2num([pd.to_datetime(trade['Date'])])[0]\n",
    "            if trade_date in date_render_range:\n",
    "                if trade['Type'] == 'buy':\n",
    "                    high_low = trade['Low']-10\n",
    "                    self.ax1.scatter(trade_date, high_low, c='red', label='red', s = 120, edgecolors='none', marker=\"^\")\n",
    "                else:\n",
    "                    high_low = trade['High']+10\n",
    "                    self.ax1.scatter(trade_date, high_low, c='blue', label='blue', s = 120, edgecolors='none', marker=\"v\")\n",
    "\n",
    "        # we need to set layers every step, because we are clearing subplots every step\n",
    "        self.ax2.set_xlabel('Date')\n",
    "        self.ax1.set_ylabel('Price')\n",
    "        self.ax3.yaxis.set_label_position('right')\n",
    "        self.ax3.set_ylabel('Balance') # 여기 수정\n",
    "\n",
    "        # I use tight_layout to replace plt.subplots_adjust\n",
    "        self.fig.tight_layout()\n",
    "\n",
    "        \"\"\"Display image with matplotlib - interrupting other tasks\"\"\"\n",
    "        # Show the graph without blocking the rest of the program\n",
    "        plt.show(block=False)\n",
    "        # Necessary to view frames before they are unrendered\n",
    "        plt.pause(0.001)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingEnv:\n",
    "    def __init__(self, df, initial_balance=1000, lookback_window_size=50, render_range=100):\n",
    "        self.df = df.dropna().reset_index()\n",
    "        self.df_total_steps = len(self.df) - 1 # 따라서 trading 데이터를 df에 넣어줘야 한다\n",
    "        self.initial_balance = initial_balance\n",
    "        self.lookback_window_size = lookback_window_size\n",
    "        self.render_range = render_range\n",
    "\n",
    "        self.buffer = RolloutBuffer()\n",
    "\n",
    "        # Neural networks part bellow\n",
    "        self.lr = 0.0001\n",
    "        self.epochs = 1\n",
    "        self.normalise_value = 100000       \n",
    "        self.batch_size = 64 \n",
    "        self.gamma = 0.99\n",
    "        self.n_epochs = 10\n",
    "        self.lmbda = 0.5\n",
    "        self.clip_ratio = 0.2\n",
    "\n",
    "        self.action_space = gym.spaces.Box(low=-1, high=1, shape=(1,))\n",
    "        self.state_size = (self.lookback_window_size, 10)\n",
    "\n",
    "        self.orders_history = deque(maxlen=self.lookback_window_size)\n",
    "        self.market_history = deque(maxlen=self.lookback_window_size)\n",
    "\n",
    "        # Create actor-critic network model\n",
    "        self.Actor = Actor().to(device)\n",
    "        self.Critic = Critic().to(device)\n",
    "\n",
    "        self.actor_optimizer = optim.Adam(self.Actor.parameters(), lr=self.lr)\n",
    "        self.critic_optimizer = optim.Adam(self.Critic.parameters(), lr=self.lr)\n",
    "\n",
    "    def reset(self, env_steps_size = 0):\n",
    "        self.balance = self.initial_balance\n",
    "        self.net_worth = self.initial_balance\n",
    "        self.prev_net_worth = self.initial_balance\n",
    "        self.crypto_held = 0\n",
    "        self.crypto_sold = 0\n",
    "        self.crypto_bought = 0\n",
    "        \n",
    "        self.visualisation = TradingGraph(render_range=self.render_range)\n",
    "        self.trades = deque(maxlen=self.render_range)\n",
    "\n",
    "        self.env_steps_size = env_steps_size\n",
    "\n",
    "        if env_steps_size > 0: # used for training dataset\n",
    "            self.start_step = random.randint(self.lookback_window_size, self.df_total_steps - env_steps_size)\n",
    "            self.end_step = self.start_step + env_steps_size\n",
    "        else: # used for testing dataset\n",
    "            self.start_step = self.lookback_window_size\n",
    "            self.end_step = self.df_total_steps\n",
    "\n",
    "        self.current_step = self.start_step\n",
    "\n",
    "        # 50일간의 데이터를 추가한다.\n",
    "        for i in reversed(range(self.lookback_window_size)):\n",
    "            current_step = self.current_step - i\n",
    "            self.orders_history.append([self.balance, self.net_worth, self.crypto_bought, self.crypto_sold, self.crypto_held])\n",
    "            self.market_history.append([self.df.loc[current_step, 'open'],\n",
    "                            self.df.loc[current_step, 'high'],\n",
    "                            self.df.loc[current_step, 'low'],\n",
    "                            self.df.loc[current_step, 'close'],\n",
    "                            self.df.loc[current_step, 'volume']\n",
    "                            ])\n",
    "        state = np.concatenate((self.market_history, self.orders_history), axis = 1)\n",
    "        return state\n",
    "\n",
    "    def act(self, state, testmode=False):\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            mu, std = self.Actor(state)\n",
    "            m = Normal(mu, std)\n",
    "            if testmode==False:\n",
    "                action = torch.normal(mean=mu, std=std)\n",
    "            else:\n",
    "                action = mu\n",
    "            log_prob_old = m.log_prob(action)\n",
    "\n",
    "        action = torch.clamp(action, -1.0, 1.0)\n",
    "        action = action.cpu()\n",
    "        log_prob_old = log_prob_old.cpu()\n",
    "        return action, log_prob_old\n",
    "\n",
    "    def _next_observation(self):\n",
    "        self.market_history.append([self.df.loc[self.current_step, 'open'],\n",
    "                                    self.df.loc[self.current_step, 'high'],\n",
    "                                    self.df.loc[self.current_step, 'low'],\n",
    "                                    self.df.loc[self.current_step, 'close'],\n",
    "                                    self.df.loc[self.current_step, 'volume']\n",
    "                                    ])\n",
    "        obs = np.concatenate((self.market_history, self.orders_history), axis=1)\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        action = action.item()\n",
    "        self.crypto_bought = 0\n",
    "        self.crypto_sold = 0\n",
    "        self.current_step += 1\n",
    "\n",
    "        date = self.df.loc[self.current_step, 'date_open']\n",
    "        high = self.df.loc[self.current_step, 'high']\n",
    "        low = self.df.loc[self.current_step, 'low']\n",
    "\n",
    "        # Set the current price to a random price between open and close\n",
    "        current_price = random.uniform(\n",
    "            self.df.loc[self.current_step, 'open'],\n",
    "            self.df.loc[self.current_step, 'close'])\n",
    "\n",
    "#        action = torch.clamp(action, -1.0, 1.0)\n",
    "        #action = action.cpu()\n",
    "        \n",
    "        if action == 0: # hold\n",
    "            pass\n",
    "\n",
    "        elif action > 0 and self.balance > self.initial_balance/100: # buy\n",
    "            action = min(action, 1)\n",
    "            self.crypto_bought = self.balance * action / current_price\n",
    "            self.balance -= self.crypto_bought * current_price\n",
    "            self.crypto_held += self.crypto_bought\n",
    "            self.trades.append({'Date': date, 'High': high, 'Low': low, 'Total': self.crypto_bought, 'Type': 'buy' })\n",
    "        \n",
    "        elif action < 0 and self.crypto_held > 0: # sell\n",
    "            action = max(action, -1)\n",
    "            self.crypto_sold = self.crypto_held * abs(action) \n",
    "            self.balance += self.crypto_sold * abs(action) * current_price\n",
    "            self.crypto_held -= self.crypto_sold\n",
    "            self.trades.append({'Date': date, 'High': high, 'Low': low, 'Total': self.crypto_bought, 'Type': 'sell' })\n",
    "\n",
    "        self.prev_net_worth = self.net_worth\n",
    "        self.net_worth = self.balance + self.crypto_held * current_price\n",
    "\n",
    "        self.orders_history.append([self.balance, self.net_worth, self.crypto_bought, self.crypto_sold, self.crypto_held])\n",
    "\n",
    "        reward = self.net_worth - self.prev_net_worth\n",
    "\n",
    "        if self.net_worth <= self.initial_balance / 2: \n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        obs = self._next_observation() #/ self.normalise_value\n",
    "\n",
    "\n",
    "        return obs, reward, done\n",
    "    \n",
    "    def render(self, visualise=False):\n",
    "        '''\n",
    "        Usually, we want to see how our agent learns, performs, etc. So we need to create a render function. \n",
    "        For simplicity's sake, we will render the current step of our environment and the net worth so far.\n",
    "        '''\n",
    "        #print(f\"Step: {self.current_step}, Net worth: {self.net_worth}\")\n",
    "        if visualise:\n",
    "            date = self.df.loc[self.current_step, 'date_open']\n",
    "            open = self.df.loc[self.current_step, 'open']\n",
    "            close = self.df.loc[self.current_step, 'close']\n",
    "            high = self.df.loc[self.current_step, 'high']\n",
    "            low = self.df.loc[self.current_step, 'low']\n",
    "            volume = self.df.loc[self.current_step, 'volume']\n",
    "\n",
    "            self.visualisation.render(date, open, high, low, close, volume, self.net_worth, self.trades)\n",
    "\n",
    "#    def get_gaes(self, rewards, dones, values, next_values, gamma = 0.99, lamda = 0.95, normalize=False):\n",
    "#        deltas = [r + gamma * (1 - d) * nv - v for r, d, nv, v in zip(rewards, dones, next_values, values)]\n",
    "#        deltas = np.stack(deltas)\n",
    "#        gaes = copy.deepcopy(deltas)\n",
    "#        for t in reversed(range(len(deltas) - 1)):\n",
    "#            gaes[t] = gaes[t] + (1 - dones[t]) * gamma * lamda * gaes[t + 1]\n",
    "#\n",
    "#        target = gaes + values\n",
    "#        #if normalize:\n",
    "#        #    gaes = (gaes - gaes.mean()) / (gaes.std() + 1e-8)\n",
    "#        return np.vstack(gaes), np.vstack(target)\n",
    "\n",
    "#    def replay(self, states, actions, rewards, predictions, dones, next_states):\n",
    "#        # reshape memory to appropriate shape for training\n",
    "#        states = np.vstack(states)\n",
    "#        next_states = np.vstack(next_states)\n",
    "#        actions = np.vstack(actions)\n",
    "#        predictions = np.vstack(predictions)\n",
    "#\n",
    "#        # Compute discounted rewards\n",
    "#        #discounted_r = np.vstack(self.discount_rewards(rewards))\n",
    "#\n",
    "#        # Get Critic network predictions \n",
    "#        values = self.critic.predict(states)\n",
    "#        next_values = self.critic.predict(next_states)\n",
    "#        \n",
    "#        # Compute advantages\n",
    "#        #advantages = discounted_r - values\n",
    "#        advantages, target = self.get_gaes(rewards, dones, np.squeeze(values), np.squeeze(next_values))\n",
    "#\n",
    "#        # stack everything to numpy array\n",
    "#        y_true = np.hstack([advantages, actions])\n",
    "#        \n",
    "#        # training Actor and Critic networks\n",
    "#        a_loss = self.actor.actor.fit(states, y_true, epochs=self.epochs, verbose=0, shuffle=True)\n",
    "#        c_loss = self.critic.critic.fit(states, target, epochs=self.epochs, verbose=0, shuffle=True)\n",
    "    \n",
    "\n",
    "    def optimize_model(self, states, actions, rewards, dones, next_states, log_prob_olds):\n",
    "        # 현재 T 내의 모든 state, action, reward, done, next_state 확보했음\n",
    "        # training_batch_size = len(states)\n",
    "        returns = [0 for _ in range(len(states))]\n",
    "        returns[0] = rewards[0]\n",
    "        for i in range(1, len(states)):\n",
    "            returns[i] += rewards[i] + returns[i-1]\n",
    "\n",
    "        deltas = [0 for _ in range(len(states)-1)]\n",
    "        gaes = [0 for _ in range(len(states)-1)]\n",
    "        self.Actor.train() # training 모드로 변경\n",
    "        self.Critic.train() # training 모드로 변경\n",
    "\n",
    "        #state_2 = torch.tensor(states[-2], dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        next_states[-2] = torch.tensor(next_states[-2], dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        states[-2] = torch.tensor(states[-2], dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        # 맨 마지막 항 먼저 구하고 for문으로 돌리면 된다\n",
    "        deltas[-1] = rewards[-2] + self.gamma * self.Critic(next_states[-2]) - self.Critic(states[-2])\n",
    "        gaes[-1] = deltas[-1]\n",
    "        for t in reversed(range(len(deltas)-1)):\n",
    "            next_states[t] = torch.tensor(next_states[t], dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            states[t] = torch.tensor(states[t], dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            # Compute advantage estimates based on the current critic\n",
    "            deltas[t] = rewards[t] + self.gamma * self.Critic(next_states[t]) - self.Critic(states[t])\n",
    "            gaes[t] = deltas[t] + deltas[t+1] * self.gamma * self.lmbda\n",
    "        \n",
    "        gaes.append(0)\n",
    "        \n",
    "        #states = torch.from_numpy(np.array(states)).to(torch.float32).to(device)\n",
    "        #actions = torch.from_numpy(np.array(actions)).to(torch.float32).to(device)\n",
    "        #returns = torch.from_numpy(np.array(returns)).to(torch.float32).to(device)\n",
    "        #gaes = torch.from_numpy(np.array(gaes)).to(torch.float32).to(device)\n",
    "        #log_prob_olds = torch.from_numpy(np.array(log_prob_olds)).to(torch.float32).to(device)\n",
    "\n",
    "        states = torch.cat\n",
    "\n",
    "        dts = TensorDataset(states, actions, returns, gaes, log_prob_olds)\n",
    "        loader = DataLoader(dts, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        for _ in range(self.n_epochs):\n",
    "            critic_losses, actor_losses, entropy_bonuses = [], [], []\n",
    "            for batch in loader:\n",
    "                state, action, ret, gae, log_prob_old = batch\n",
    "                value = self.Critic(state)\n",
    "                critic_loss = F.mse_loss(value, ret)\n",
    "                self.critic_optimizer.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                self.critic_optimizer.step()\n",
    "\n",
    "                mu, std = self.Actor(state)\n",
    "                m = Normal(mu, std)\n",
    "                action = torch.normal(mean=mu, std=std)\n",
    "                log_prob = m.log_prob(action)\n",
    "\n",
    "                ratio = (log_prob - log_prob_old).exp()\n",
    "                surr1 = gae * ratio\n",
    "                surr2 = gae * torch.clamp(ratio, 1.0 - self.clip_ratio, 1.0 + self.clip_ratio)\n",
    "\n",
    "                actor_loss = -torch.min(surr1, surr2).mean()\n",
    "                entropy_bonus = -m.entropy().mean()\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                self.actor_optimizer.step()\n",
    "\n",
    "                critic_losses.append(critic_loss.item())\n",
    "                actor_losses.append(actor_loss.item())\n",
    "                entropy_bonuses.append(entropy_bonus.item())\n",
    "        \n",
    "        result = {'actor_loss': np.mean(actor_losses),\n",
    "                  'critic_loss': np.mean(critic_losses),\n",
    "                  'entropy_bonus': np.mean(entropy_bonuses)}\n",
    "        \n",
    "        return result\n",
    "        \n",
    "\n",
    "    def save(self, name=\"ppo\"):\n",
    "        torch.save(self.Actor.state_dict(), f'./ppo_copy/{name}_Actor.h5')\n",
    "        torch.save(self.Critic.state_dict(), f'./ppo_copy/{name}_Critic.h5')\n",
    "\n",
    "    def load(self, name=\"ppo\"):\n",
    "        self.Actor.load_state_dict(torch.load(f'./ppo_copy/{name}_Actor.h5', weights_only=True))\n",
    "        self.Critic.load_state_dict(torch.load(f'./ppo_copy/{name}_Critic.h5', weights_only=True))\n",
    "\n",
    "\n",
    "def train_agent(env, visualize=False, train_episodes=20, training_batch_size=500):\n",
    "    #memory = Memory(training_batch_size)\n",
    "\n",
    "    for episode in range(train_episodes):\n",
    "        state = env.reset(env_steps_size = training_batch_size)\n",
    "        #state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "        # Create episode minibatch\n",
    "        states, actions, rewards, dones, next_states, log_prob_olds = [], [], [], [], [], []\n",
    "        for _ in range(training_batch_size):\n",
    "            env.render(visualize)\n",
    "            #states.append(np.expand_dims(state, axis=0))\n",
    "            # Selcet action\n",
    "            #state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            action, log_prob_old = env.act(state, testmode=False) \n",
    "            # Observe next state, reward and done signal\n",
    "            next_state, reward, done = env.step(action)\n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            #memory.push(state, action, next_state, reward)\n",
    "            # Store (next_state, action, reward) in the episode minibatch\n",
    "            states.append(np.expand_dims(state, axis=0))\n",
    "            next_states.append(np.expand_dims(next_state, axis=0))\n",
    "            #next_state = torch.tensor(next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            dones.append(done)\n",
    "            log_prob_olds.append(log_prob_old)\n",
    "            \n",
    "            # Update state\n",
    "            state = next_state\n",
    "        \n",
    "        # Timestep 내의 모든 state, action, reward, done, next_state 확보했음\n",
    "        # 확보한 애들을 optimise에 던져줘야 함\n",
    "        env.optimize_model(states, actions, rewards, dones, next_states, log_prob_olds)\n",
    "        print(f\"Episode {episode} net_worth: {env.net_worth}\")\n",
    "        if episode == train_episodes - 1:\n",
    "            env.save()\n",
    "\n",
    "\n",
    "def test_agent(env, visualize=True, test_episodes=10):\n",
    "    env.load() # load the model\n",
    "    average_net_worth = 0\n",
    "    for episode in range(test_episodes):\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            env.render(visualize)\n",
    "            action, prediction = env.act(state)\n",
    "            state, reward, done = env.step(action)\n",
    "            if env.current_step == env.end_step:\n",
    "                average_net_worth += env.net_worth\n",
    "                break\n",
    "            \n",
    "    print(\"average {} episodes agent net_worth: {}\".format(test_episodes, average_net_worth/test_episodes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback_window_size = 50\n",
    "data_path = \"./data/binance-BTCUSDT-1h.pkl\"\n",
    "df = pd.read_pickle(data_path)\n",
    "train_size = int(len(df) * 0.7)\n",
    "train_df = df[:train_size]\n",
    "test_df = df[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = TradingEnv(train_df, lookback_window_size=lookback_window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x800 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "state = train_env.reset(env_steps_size = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions, rewards, dones, next_states, log_prob_olds = [], [], [], [], [], []\n",
    "for _ in range(500):\n",
    "    #states.append(np.expand_dims(state, axis=0))\n",
    "    # Selcet action\n",
    "    #state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    action, log_prob_old = train_env.act(state, testmode=False) \n",
    "    # Observe next state, reward and done signal\n",
    "    next_state, reward, done = train_env.step(action)\n",
    "    if done:\n",
    "        break\n",
    "    \n",
    "    #memory.push(state, action, next_state, reward)\n",
    "    # Store (next_state, action, reward) in the episode minibatch\n",
    "    states.append(np.expand_dims(state, axis=0))\n",
    "    next_states.append(np.expand_dims(next_state, axis=0))\n",
    "    #next_state = torch.tensor(next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    actions.append(action)\n",
    "    rewards.append(reward)\n",
    "    dones.append(done)\n",
    "    log_prob_olds.append(log_prob_old)\n",
    "    \n",
    "    # Update state\n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstates\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "states.device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 1, 50, 10])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states = torch.from_numpy(np.array(states)).to(torch.float32).to(device)\n",
    "states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.2173]]),\n",
       " tensor([[-0.0747]]),\n",
       " tensor([[-0.5033]]),\n",
       " tensor([[-0.8799]]),\n",
       " tensor([[0.0507]]),\n",
       " tensor([[-0.0860]]),\n",
       " tensor([[-0.0822]]),\n",
       " tensor([[-1.0333]]),\n",
       " tensor([[-0.0175]]),\n",
       " tensor([[-0.0020]]),\n",
       " tensor([[-0.4157]]),\n",
       " tensor([[0.0733]]),\n",
       " tensor([[0.0304]]),\n",
       " tensor([[-0.0782]]),\n",
       " tensor([[0.0698]]),\n",
       " tensor([[-0.4075]]),\n",
       " tensor([[0.0632]]),\n",
       " tensor([[-0.7181]]),\n",
       " tensor([[-0.2766]]),\n",
       " tensor([[0.0744]]),\n",
       " tensor([[-0.3200]]),\n",
       " tensor([[-0.2295]]),\n",
       " tensor([[-0.2206]]),\n",
       " tensor([[-0.5591]]),\n",
       " tensor([[-0.0959]]),\n",
       " tensor([[-0.1610]]),\n",
       " tensor([[-0.9175]]),\n",
       " tensor([[0.0075]]),\n",
       " tensor([[0.0413]]),\n",
       " tensor([[0.0602]]),\n",
       " tensor([[-0.4915]]),\n",
       " tensor([[0.0313]]),\n",
       " tensor([[0.0495]]),\n",
       " tensor([[-2.9622]]),\n",
       " tensor([[-0.0403]]),\n",
       " tensor([[-0.8113]]),\n",
       " tensor([[-0.0536]]),\n",
       " tensor([[0.0571]]),\n",
       " tensor([[-0.6935]]),\n",
       " tensor([[-0.5980]]),\n",
       " tensor([[0.0637]]),\n",
       " tensor([[-0.2705]]),\n",
       " tensor([[-0.1047]]),\n",
       " tensor([[-0.2627]]),\n",
       " tensor([[-0.9896]]),\n",
       " tensor([[-1.8295]]),\n",
       " tensor([[-1.2826]]),\n",
       " tensor([[0.0710]]),\n",
       " tensor([[-0.0807]]),\n",
       " tensor([[-1.5338]]),\n",
       " tensor([[-0.5250]]),\n",
       " tensor([[-1.0263]]),\n",
       " tensor([[0.0791]]),\n",
       " tensor([[-0.1659]]),\n",
       " tensor([[-0.1680]]),\n",
       " tensor([[0.0227]]),\n",
       " tensor([[-0.8593]]),\n",
       " tensor([[0.0066]]),\n",
       " tensor([[0.0798]]),\n",
       " tensor([[-0.9087]]),\n",
       " tensor([[-0.7949]]),\n",
       " tensor([[-1.2413]]),\n",
       " tensor([[0.0800]]),\n",
       " tensor([[-0.0652]]),\n",
       " tensor([[0.0757]]),\n",
       " tensor([[0.0653]]),\n",
       " tensor([[0.0793]]),\n",
       " tensor([[-0.0963]]),\n",
       " tensor([[-0.0478]]),\n",
       " tensor([[-0.0461]]),\n",
       " tensor([[-0.1803]]),\n",
       " tensor([[-0.4494]]),\n",
       " tensor([[-0.1485]]),\n",
       " tensor([[-0.0873]]),\n",
       " tensor([[-0.0314]]),\n",
       " tensor([[0.0726]]),\n",
       " tensor([[-0.1221]]),\n",
       " tensor([[0.0697]]),\n",
       " tensor([[-0.4573]]),\n",
       " tensor([[-1.6491]]),\n",
       " tensor([[0.0716]]),\n",
       " tensor([[-0.5674]]),\n",
       " tensor([[-0.1900]]),\n",
       " tensor([[-0.4118]]),\n",
       " tensor([[-0.1257]]),\n",
       " tensor([[-0.0265]]),\n",
       " tensor([[0.0466]]),\n",
       " tensor([[-0.0117]]),\n",
       " tensor([[0.0742]]),\n",
       " tensor([[-1.1073]]),\n",
       " tensor([[-2.0290]]),\n",
       " tensor([[-0.0717]]),\n",
       " tensor([[-0.6726]]),\n",
       " tensor([[-0.0104]]),\n",
       " tensor([[0.0760]]),\n",
       " tensor([[-0.2147]]),\n",
       " tensor([[-0.3512]]),\n",
       " tensor([[-0.0052]]),\n",
       " tensor([[0.0742]]),\n",
       " tensor([[0.0490]]),\n",
       " tensor([[0.0793]]),\n",
       " tensor([[0.0766]]),\n",
       " tensor([[-0.1528]]),\n",
       " tensor([[0.0785]]),\n",
       " tensor([[-0.6612]]),\n",
       " tensor([[-2.9011]]),\n",
       " tensor([[-0.3652]]),\n",
       " tensor([[0.0649]]),\n",
       " tensor([[-0.0281]]),\n",
       " tensor([[0.0545]]),\n",
       " tensor([[-0.4669]]),\n",
       " tensor([[-0.5364]]),\n",
       " tensor([[-1.8477]]),\n",
       " tensor([[0.0785]]),\n",
       " tensor([[-0.0647]]),\n",
       " tensor([[-0.0303]]),\n",
       " tensor([[0.0531]]),\n",
       " tensor([[-0.3971]]),\n",
       " tensor([[-0.0636]]),\n",
       " tensor([[-0.0301]]),\n",
       " tensor([[0.0680]]),\n",
       " tensor([[-0.1086]]),\n",
       " tensor([[0.0768]]),\n",
       " tensor([[0.0682]]),\n",
       " tensor([[-0.0824]]),\n",
       " tensor([[0.0793]]),\n",
       " tensor([[-1.5386]]),\n",
       " tensor([[0.0230]]),\n",
       " tensor([[-0.1025]]),\n",
       " tensor([[0.0754]]),\n",
       " tensor([[0.0151]]),\n",
       " tensor([[-0.0816]]),\n",
       " tensor([[0.0648]]),\n",
       " tensor([[-1.4753]]),\n",
       " tensor([[0.0791]]),\n",
       " tensor([[-0.2212]]),\n",
       " tensor([[0.0158]]),\n",
       " tensor([[0.0620]]),\n",
       " tensor([[0.0754]]),\n",
       " tensor([[-0.1202]]),\n",
       " tensor([[-0.1763]]),\n",
       " tensor([[-0.1348]]),\n",
       " tensor([[-2.9992]]),\n",
       " tensor([[-0.1221]]),\n",
       " tensor([[0.0088]]),\n",
       " tensor([[0.0772]]),\n",
       " tensor([[-1.0960]]),\n",
       " tensor([[0.0582]]),\n",
       " tensor([[0.0600]]),\n",
       " tensor([[0.0710]]),\n",
       " tensor([[-0.3374]]),\n",
       " tensor([[0.0118]]),\n",
       " tensor([[-1.6944]]),\n",
       " tensor([[-1.2338]]),\n",
       " tensor([[-0.6710]]),\n",
       " tensor([[-0.1505]]),\n",
       " tensor([[-0.0910]]),\n",
       " tensor([[0.0010]]),\n",
       " tensor([[0.0739]]),\n",
       " tensor([[-0.6245]]),\n",
       " tensor([[-1.2209]]),\n",
       " tensor([[-0.3689]]),\n",
       " tensor([[-1.0737]]),\n",
       " tensor([[-0.7367]]),\n",
       " tensor([[0.0728]]),\n",
       " tensor([[0.0783]]),\n",
       " tensor([[-0.4687]]),\n",
       " tensor([[-0.2905]]),\n",
       " tensor([[0.0724]]),\n",
       " tensor([[-0.3997]]),\n",
       " tensor([[-3.1005]]),\n",
       " tensor([[-1.0442]]),\n",
       " tensor([[-0.1072]]),\n",
       " tensor([[-1.5326]]),\n",
       " tensor([[-0.8776]]),\n",
       " tensor([[-0.2382]]),\n",
       " tensor([[0.0762]]),\n",
       " tensor([[0.0113]]),\n",
       " tensor([[-0.6641]]),\n",
       " tensor([[0.0403]]),\n",
       " tensor([[-1.9540]]),\n",
       " tensor([[-0.3941]]),\n",
       " tensor([[-1.0854]]),\n",
       " tensor([[-0.1544]]),\n",
       " tensor([[-0.0239]]),\n",
       " tensor([[-1.7754]]),\n",
       " tensor([[-0.9208]]),\n",
       " tensor([[-0.0152]]),\n",
       " tensor([[-1.1689]]),\n",
       " tensor([[0.0723]]),\n",
       " tensor([[-0.0077]]),\n",
       " tensor([[0.0809]]),\n",
       " tensor([[-0.0466]]),\n",
       " tensor([[-0.0668]]),\n",
       " tensor([[-0.0895]]),\n",
       " tensor([[0.0805]]),\n",
       " tensor([[-1.6670]]),\n",
       " tensor([[-0.8675]]),\n",
       " tensor([[-0.0559]]),\n",
       " tensor([[-1.6312]]),\n",
       " tensor([[-0.2994]]),\n",
       " tensor([[-0.5731]]),\n",
       " tensor([[0.0721]]),\n",
       " tensor([[-0.8818]]),\n",
       " tensor([[0.0793]]),\n",
       " tensor([[-0.0392]]),\n",
       " tensor([[0.0662]]),\n",
       " tensor([[-0.1139]]),\n",
       " tensor([[0.0449]]),\n",
       " tensor([[0.0413]]),\n",
       " tensor([[-0.9302]]),\n",
       " tensor([[-0.2248]]),\n",
       " tensor([[0.0666]]),\n",
       " tensor([[0.0748]]),\n",
       " tensor([[0.0805]]),\n",
       " tensor([[-0.1562]]),\n",
       " tensor([[-0.3730]]),\n",
       " tensor([[-0.6432]]),\n",
       " tensor([[-0.1820]]),\n",
       " tensor([[-1.1632]]),\n",
       " tensor([[0.0808]]),\n",
       " tensor([[0.0098]]),\n",
       " tensor([[-0.0743]]),\n",
       " tensor([[-0.3462]]),\n",
       " tensor([[-0.2860]]),\n",
       " tensor([[0.0787]]),\n",
       " tensor([[-1.3402]]),\n",
       " tensor([[-0.3903]]),\n",
       " tensor([[-0.8219]]),\n",
       " tensor([[-0.1098]]),\n",
       " tensor([[-0.6767]]),\n",
       " tensor([[-1.8399]]),\n",
       " tensor([[-1.0392]]),\n",
       " tensor([[-0.2792]]),\n",
       " tensor([[-0.6849]]),\n",
       " tensor([[0.0627]]),\n",
       " tensor([[-0.1253]]),\n",
       " tensor([[-0.4432]]),\n",
       " tensor([[0.0463]]),\n",
       " tensor([[0.0799]]),\n",
       " tensor([[-0.0647]]),\n",
       " tensor([[-1.4041]]),\n",
       " tensor([[-0.3586]]),\n",
       " tensor([[-0.1737]]),\n",
       " tensor([[0.0682]]),\n",
       " tensor([[-0.2160]]),\n",
       " tensor([[-0.1332]]),\n",
       " tensor([[0.0719]]),\n",
       " tensor([[0.0638]]),\n",
       " tensor([[-0.2797]]),\n",
       " tensor([[-0.3871]]),\n",
       " tensor([[0.0776]]),\n",
       " tensor([[-0.3108]]),\n",
       " tensor([[0.0320]]),\n",
       " tensor([[-0.7401]]),\n",
       " tensor([[-0.6241]]),\n",
       " tensor([[-0.1937]]),\n",
       " tensor([[-1.1889]]),\n",
       " tensor([[-0.4645]]),\n",
       " tensor([[0.0179]]),\n",
       " tensor([[-0.7405]]),\n",
       " tensor([[-0.7131]]),\n",
       " tensor([[0.0784]]),\n",
       " tensor([[-0.3642]]),\n",
       " tensor([[-1.0155]]),\n",
       " tensor([[-0.0634]]),\n",
       " tensor([[-0.2752]]),\n",
       " tensor([[0.0735]]),\n",
       " tensor([[-0.0765]]),\n",
       " tensor([[0.0800]]),\n",
       " tensor([[0.0379]]),\n",
       " tensor([[-1.4906]]),\n",
       " tensor([[-0.4687]]),\n",
       " tensor([[-0.5254]]),\n",
       " tensor([[-0.1394]]),\n",
       " tensor([[-0.1272]]),\n",
       " tensor([[-0.4032]]),\n",
       " tensor([[-1.2986]]),\n",
       " tensor([[-0.3727]]),\n",
       " tensor([[-0.8170]]),\n",
       " tensor([[-0.6985]]),\n",
       " tensor([[-0.4483]]),\n",
       " tensor([[-0.0172]]),\n",
       " tensor([[-0.3093]]),\n",
       " tensor([[-1.4254]]),\n",
       " tensor([[0.0807]]),\n",
       " tensor([[0.0811]]),\n",
       " tensor([[-0.6241]]),\n",
       " tensor([[0.0303]]),\n",
       " tensor([[-0.1830]]),\n",
       " tensor([[-0.2130]]),\n",
       " tensor([[-1.7423]]),\n",
       " tensor([[-0.0217]]),\n",
       " tensor([[-0.4816]]),\n",
       " tensor([[-3.3713]]),\n",
       " tensor([[-0.8552]]),\n",
       " tensor([[0.0803]]),\n",
       " tensor([[-1.5000]]),\n",
       " tensor([[0.0179]]),\n",
       " tensor([[-0.0721]]),\n",
       " tensor([[0.0369]]),\n",
       " tensor([[-0.2764]]),\n",
       " tensor([[0.0088]]),\n",
       " tensor([[-0.7723]]),\n",
       " tensor([[-0.0168]]),\n",
       " tensor([[-1.1916]]),\n",
       " tensor([[-0.5445]]),\n",
       " tensor([[-0.1185]]),\n",
       " tensor([[-0.1380]]),\n",
       " tensor([[-0.9828]]),\n",
       " tensor([[0.0427]]),\n",
       " tensor([[0.0174]]),\n",
       " tensor([[0.0006]]),\n",
       " tensor([[-0.3044]]),\n",
       " tensor([[-0.0403]]),\n",
       " tensor([[0.0810]]),\n",
       " tensor([[-0.3058]]),\n",
       " tensor([[0.0634]]),\n",
       " tensor([[-1.1842]]),\n",
       " tensor([[0.0198]]),\n",
       " tensor([[-0.2392]]),\n",
       " tensor([[-0.0134]]),\n",
       " tensor([[-0.0536]]),\n",
       " tensor([[0.0740]]),\n",
       " tensor([[-1.5452]]),\n",
       " tensor([[-0.1592]]),\n",
       " tensor([[-0.0010]]),\n",
       " tensor([[-0.2811]]),\n",
       " tensor([[-0.8210]]),\n",
       " tensor([[0.0804]]),\n",
       " tensor([[-0.8855]]),\n",
       " tensor([[-0.1398]]),\n",
       " tensor([[-0.3316]]),\n",
       " tensor([[0.0690]]),\n",
       " tensor([[0.0413]]),\n",
       " tensor([[-0.0212]]),\n",
       " tensor([[-1.6969]]),\n",
       " tensor([[-0.2564]]),\n",
       " tensor([[0.0394]]),\n",
       " tensor([[-0.2817]]),\n",
       " tensor([[-0.3768]]),\n",
       " tensor([[0.0043]]),\n",
       " tensor([[-0.0272]]),\n",
       " tensor([[-0.5412]]),\n",
       " tensor([[-0.2559]]),\n",
       " tensor([[-0.0399]]),\n",
       " tensor([[-0.4757]]),\n",
       " tensor([[-1.7850]]),\n",
       " tensor([[0.0079]]),\n",
       " tensor([[-0.2188]]),\n",
       " tensor([[0.0807]]),\n",
       " tensor([[0.0778]]),\n",
       " tensor([[-0.0077]]),\n",
       " tensor([[0.0303]]),\n",
       " tensor([[-1.5041]]),\n",
       " tensor([[0.0152]]),\n",
       " tensor([[-0.5995]]),\n",
       " tensor([[-0.3472]]),\n",
       " tensor([[-0.2300]]),\n",
       " tensor([[-0.1298]]),\n",
       " tensor([[-0.2137]]),\n",
       " tensor([[-1.2364]]),\n",
       " tensor([[-0.3404]]),\n",
       " tensor([[-2.9247]]),\n",
       " tensor([[-0.1253]]),\n",
       " tensor([[-1.2252]]),\n",
       " tensor([[-0.0910]]),\n",
       " tensor([[0.0803]]),\n",
       " tensor([[0.0662]]),\n",
       " tensor([[0.0773]]),\n",
       " tensor([[-0.7623]]),\n",
       " tensor([[-0.1922]]),\n",
       " tensor([[0.0793]]),\n",
       " tensor([[-0.0463]]),\n",
       " tensor([[-0.9468]]),\n",
       " tensor([[-2.5165]]),\n",
       " tensor([[-0.1592]]),\n",
       " tensor([[-0.2165]]),\n",
       " tensor([[0.0366]]),\n",
       " tensor([[-1.0452]]),\n",
       " tensor([[0.0756]]),\n",
       " tensor([[-1.3695]]),\n",
       " tensor([[-1.6639]]),\n",
       " tensor([[0.0474]]),\n",
       " tensor([[-0.3696]]),\n",
       " tensor([[-0.0648]]),\n",
       " tensor([[0.0071]]),\n",
       " tensor([[-0.6170]]),\n",
       " tensor([[-0.1005]]),\n",
       " tensor([[-1.1800]]),\n",
       " tensor([[-0.5214]]),\n",
       " tensor([[-0.2502]]),\n",
       " tensor([[0.0142]]),\n",
       " tensor([[-0.3444]]),\n",
       " tensor([[-0.0028]]),\n",
       " tensor([[-2.6467]]),\n",
       " tensor([[0.0018]]),\n",
       " tensor([[-0.1689]]),\n",
       " tensor([[0.0723]]),\n",
       " tensor([[-0.2573]]),\n",
       " tensor([[-0.6159]]),\n",
       " tensor([[0.0793]]),\n",
       " tensor([[0.0808]]),\n",
       " tensor([[-0.6528]]),\n",
       " tensor([[-0.4994]]),\n",
       " tensor([[-0.7645]]),\n",
       " tensor([[-0.9378]]),\n",
       " tensor([[-2.2636]]),\n",
       " tensor([[0.0109]]),\n",
       " tensor([[0.0616]]),\n",
       " tensor([[-0.3117]]),\n",
       " tensor([[0.0463]]),\n",
       " tensor([[-2.1556]]),\n",
       " tensor([[-0.5335]]),\n",
       " tensor([[-0.6730]]),\n",
       " tensor([[0.0803]]),\n",
       " tensor([[-0.1092]]),\n",
       " tensor([[0.0157]]),\n",
       " tensor([[-0.9843]]),\n",
       " tensor([[-0.2928]]),\n",
       " tensor([[-0.1067]]),\n",
       " tensor([[-0.4638]]),\n",
       " tensor([[-1.1252]]),\n",
       " tensor([[-1.3124]]),\n",
       " tensor([[0.0687]]),\n",
       " tensor([[-0.4220]]),\n",
       " tensor([[-0.4621]]),\n",
       " tensor([[-1.9280]]),\n",
       " tensor([[-1.1826]]),\n",
       " tensor([[-0.1603]]),\n",
       " tensor([[-0.4463]]),\n",
       " tensor([[-0.0418]]),\n",
       " tensor([[-2.8230]]),\n",
       " tensor([[-0.8767]]),\n",
       " tensor([[0.0750]]),\n",
       " tensor([[0.0477]]),\n",
       " tensor([[-0.8882]]),\n",
       " tensor([[0.0574]]),\n",
       " tensor([[-0.9105]]),\n",
       " tensor([[-0.1666]]),\n",
       " tensor([[-0.4617]]),\n",
       " tensor([[-0.1801]]),\n",
       " tensor([[0.0410]]),\n",
       " tensor([[-0.1718]]),\n",
       " tensor([[-0.2840]]),\n",
       " tensor([[-0.1063]]),\n",
       " tensor([[-0.7606]]),\n",
       " tensor([[-0.4456]]),\n",
       " tensor([[-0.1374]]),\n",
       " tensor([[-0.1873]]),\n",
       " tensor([[-0.1859]]),\n",
       " tensor([[0.0810]]),\n",
       " tensor([[0.0135]]),\n",
       " tensor([[-1.2364]]),\n",
       " tensor([[-0.6894]]),\n",
       " tensor([[0.0245]]),\n",
       " tensor([[0.0799]]),\n",
       " tensor([[-0.1939]]),\n",
       " tensor([[-0.1859]]),\n",
       " tensor([[0.0787]]),\n",
       " tensor([[-1.9413]]),\n",
       " tensor([[-1.1842]]),\n",
       " tensor([[-2.0052]]),\n",
       " tensor([[-0.2363]]),\n",
       " tensor([[-0.5188]]),\n",
       " tensor([[-0.1512]]),\n",
       " tensor([[-0.0287]]),\n",
       " tensor([[-0.2345]]),\n",
       " tensor([[-1.3041]]),\n",
       " tensor([[-0.6657]]),\n",
       " tensor([[-0.2387]]),\n",
       " tensor([[-0.1791]]),\n",
       " tensor([[-0.8850]]),\n",
       " tensor([[0.0682]]),\n",
       " tensor([[-0.0061]]),\n",
       " tensor([[-0.1638]]),\n",
       " tensor([[0.0634]]),\n",
       " tensor([[-1.8896]]),\n",
       " tensor([[-0.7567]]),\n",
       " tensor([[-0.5629]]),\n",
       " tensor([[-0.1291]]),\n",
       " tensor([[-0.1535]]),\n",
       " tensor([[-0.0124]]),\n",
       " tensor([[-1.7696]]),\n",
       " tensor([[-0.0574]]),\n",
       " tensor([[-0.4085]]),\n",
       " tensor([[-0.1996]]),\n",
       " tensor([[-0.3023]]),\n",
       " tensor([[0.0736]]),\n",
       " tensor([[-0.1217]]),\n",
       " tensor([[-0.4167]]),\n",
       " tensor([[-1.0552]]),\n",
       " tensor([[-1.1160]]),\n",
       " tensor([[-0.0156]]),\n",
       " tensor([[-0.1152]]),\n",
       " tensor([[0.0126]]),\n",
       " tensor([[0.0803]]),\n",
       " tensor([[-0.2502]]),\n",
       " tensor([[-0.6960]]),\n",
       " tensor([[0.0800]])]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_prob_olds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = torch.cat(actions,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 1])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]]),\n",
       " tensor([[-1.]])]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = torch.from_numpy(np.array(actions)).to(torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = [0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = torch.tensor(f, dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = torch.from_numpy(np.array(f)).to(torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[111], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m states \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got numpy.ndarray"
     ]
    }
   ],
   "source": [
    "states = torch.cat(states, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdones\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_prob_olds\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[66], line 229\u001b[0m, in \u001b[0;36mTradingEnv.optimize_model\u001b[0;34m(self, states, actions, rewards, dones, next_states, log_prob_olds)\u001b[0m\n\u001b[1;32m    225\u001b[0m     gaes[t] \u001b[38;5;241m=\u001b[39m deltas[t] \u001b[38;5;241m+\u001b[39m deltas[t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlmbda\n\u001b[1;32m    227\u001b[0m gaes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 229\u001b[0m states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39marray(states))\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    230\u001b[0m actions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39marray(actions))\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    231\u001b[0m returns \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39marray(returns))\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/Desktop/MATH70101 Deep Learning/Coursework III/dl-cw-3-gpu/lib/python3.11/site-packages/torch/_tensor.py:1083\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1081\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1083\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "train_env.optimize_model(states, actions, rewards, dones, next_states, log_prob_olds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_agent(env, visualize=False, train_episodes=20, training_batch_size=500):\n",
    "    #memory = Memory(training_batch_size)\n",
    "\n",
    "    for episode in range(train_episodes):\n",
    "        state = env.reset(env_steps_size = training_batch_size)\n",
    "        #state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "        # Create episode minibatch\n",
    "        states, actions, rewards, dones, next_states, log_prob_olds = [], [], [], [], [], []\n",
    "        for _ in range(training_batch_size):\n",
    "            env.render(visualize)\n",
    "            #states.append(np.expand_dims(state, axis=0))\n",
    "            # Selcet action\n",
    "            #state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            action, log_prob_old = env.act(state, testmode=False) \n",
    "            # Observe next state, reward and done signal\n",
    "            next_state, reward, done = env.step(action)\n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            #memory.push(state, action, next_state, reward)\n",
    "            # Store (next_state, action, reward) in the episode minibatch\n",
    "            states.append(np.expand_dims(state, axis=0))\n",
    "            next_states.append(np.expand_dims(next_state, axis=0))\n",
    "            #next_state = torch.tensor(next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            dones.append(done)\n",
    "            log_prob_olds.append(log_prob_old)\n",
    "            \n",
    "            # Update state\n",
    "            state = next_state\n",
    "        \n",
    "        # Timestep 내의 모든 state, action, reward, done, next_state 확보했음\n",
    "        # 확보한 애들을 optimise에 던져줘야 함\n",
    "        env.optimize_model(states, actions, rewards, dones, next_states, log_prob_olds)\n",
    "        print(f\"Episode {episode} net_worth: {env.net_worth}\")\n",
    "        if episode == train_episodes - 1:\n",
    "            env.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-cw-3-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
